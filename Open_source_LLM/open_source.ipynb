{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7b23d0dd4c51470faaa3370a55bdd158": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9239d0fa3513439d886005d3a62ddfd2",
              "IPY_MODEL_4a596c409bb84fac9e3c86d3fce311d5",
              "IPY_MODEL_dee761500d5744b8a3328004c6905f8a"
            ],
            "layout": "IPY_MODEL_af22c72d7ac1439fa222e0cce0764b00"
          }
        },
        "9239d0fa3513439d886005d3a62ddfd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3730dabf6ad14b5c8de4a0a3bc22dd4b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f2cb43f8c72d4359b0641ae267ba7d83",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "4a596c409bb84fac9e3c86d3fce311d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6976bc3ad2e24b35925ad381f1ae60d8",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eeb3df337043487c8d0e914a18dce37d",
            "value": 4
          }
        },
        "dee761500d5744b8a3328004c6905f8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5aafb15942174ae98bc12bcc9428b88a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9fda6b8b308b486c899b879665d09c00",
            "value": "‚Äá4/4‚Äá[01:04&lt;00:00,‚Äá14.35s/it]"
          }
        },
        "af22c72d7ac1439fa222e0cce0764b00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3730dabf6ad14b5c8de4a0a3bc22dd4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2cb43f8c72d4359b0641ae267ba7d83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6976bc3ad2e24b35925ad381f1ae60d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eeb3df337043487c8d0e914a18dce37d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5aafb15942174ae98bc12bcc9428b88a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9fda6b8b308b486c899b879665d09c00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U torch transformers embed_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ep1WxEMkleB",
        "outputId": "dc40e545-6431-46e7-fc41-96a53458dee9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement embed_content (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for embed_content\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb google-generativeai langchain-text-splitters bitsandbytes accelerate\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "fx7EaPC5kwL5",
        "outputId": "7de0a987-4ace-4124-87d5-a1a04b88364d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: chromadb in /usr/local/lib/python3.12/dist-packages (1.3.5)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.12/dist-packages (0.8.5)\n",
            "Requirement already satisfied: langchain-text-splitters in /usr/local/lib/python3.12/dist-packages (1.0.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.48.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.3.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.11.10)\n",
            "Requirement already satisfied: pybase64>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.4.2)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.38.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.0.2)\n",
            "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.15.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.23.2)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.38.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.38.0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.38.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.22.1)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.76.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.0.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.20.0)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (34.1.0)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (9.1.2)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.0.3)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.2.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.11.4)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.25.1)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.28.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.187.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.43.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-text-splitters) (1.0.7)\n",
            "Collecting torch<3,>=2.3 (from bitsandbytes)\n",
            "  Using cached torch-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.36.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.7.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (1.72.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (2.32.4)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (6.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.29.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: urllib3<2.4.0,>=1.24.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.4.45)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.9.23)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.38.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.38.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.38.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.38.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.59b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.59b0)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.4.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (8.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.7.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.2.1)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.22.1)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.31.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.2.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.5)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (3.0.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.25.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Using cached torch-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl (899.7 MB)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: torch\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.24.0+cu126 requires torch==2.9.0, but you have torch 2.9.1 which is incompatible.\n",
            "torchaudio 2.9.0+cu126 requires torch==2.9.0, but you have torch 2.9.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-2.9.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from chromadb import PersistentClient\n"
      ],
      "metadata": {
        "id": "IzD7rffFkulD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# üèóÔ∏è CELL 1: SETUP & MODEL LOADING (FIXED)\n",
        "# (Run this ONCE. It takes ~2-3 minutes)\n",
        "# ==========================================\n",
        "\n",
        "# 1. Install Dependencies\n",
        "# We force a re-install of bitsandbytes to fix the 'functional' attribute error\n",
        "!pip install -q -U bitsandbytes transformers accelerate torch chromadb google-generativeai langchain-text-splitters\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
        "from huggingface_hub import login\n",
        "\n",
        "# 2. Login to Hugging Face\n",
        "# Replace with your actual token\n",
        "HF_TOKEN = \"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\"\n",
        "\n",
        "try:\n",
        "    login(token=HF_TOKEN)\n",
        "    print(\"‚úÖ Hugging Face Login Successful!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Login Failed: {e}\")\n",
        "\n",
        "# 3. Load Llama 3 Model (GPU)\n",
        "print(\"‚è≥ Loading Llama 3 (This takes a few minutes)...\")\n",
        "LLM_MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "# Config for 4-bit quantization (Fits in Colab T4 GPU)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_ID, token=HF_TOKEN)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        LLM_MODEL_ID,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        token=HF_TOKEN\n",
        "    )\n",
        "\n",
        "    # Create the pipeline (Global variable 'text_pipe' will be used in Cell 2)\n",
        "    text_pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_new_tokens=512,\n",
        "        temperature=0.1,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    print(\"‚úÖ Llama 3 Model Loaded Successfully! You can now run the next cell.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Model Load Error: {e}\")\n",
        "    print(\"Tip: If the error persists, try 'Runtime > Restart Session' again.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223,
          "referenced_widgets": [
            "7b23d0dd4c51470faaa3370a55bdd158",
            "9239d0fa3513439d886005d3a62ddfd2",
            "4a596c409bb84fac9e3c86d3fce311d5",
            "dee761500d5744b8a3328004c6905f8a",
            "af22c72d7ac1439fa222e0cce0764b00",
            "3730dabf6ad14b5c8de4a0a3bc22dd4b",
            "f2cb43f8c72d4359b0641ae267ba7d83",
            "6976bc3ad2e24b35925ad381f1ae60d8",
            "eeb3df337043487c8d0e914a18dce37d",
            "5aafb15942174ae98bc12bcc9428b88a",
            "9fda6b8b308b486c899b879665d09c00"
          ]
        },
        "id": "ZHQMz84DkGhb",
        "outputId": "782cbdc8-ac7e-42e6-d60d-f1b8cfcfc3cf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Hugging Face Login Successful!\n",
            "‚è≥ Loading Llama 3 (This takes a few minutes)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7b23d0dd4c51470faaa3370a55bdd158"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Llama 3 Model Loaded Successfully! You can now run the next cell.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# üß† CELL 2: HYBRID RAG LOGIC (FIXED DB PATH)\n",
        "# ==========================================\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import gc\n",
        "import google.generativeai as genai\n",
        "from chromadb import PersistentClient\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# --- 1. CONFIGURATION ---\n",
        "\n",
        "# üîë PASTE YOUR GOOGLE API KEY HERE\n",
        "os.environ[\"GEMINI_API_KEY\"] = \"AIzaSyDQ6wHtu6Ss67a8bjlxnlxedV1mEnERauI\"\n",
        "\n",
        "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
        "\n",
        "# UPDATE: Changed path to bypass the \"Readonly\" lock error\n",
        "DB_PATH = \"./chroma_db_fresh_v2\" # Changed to a new fresh path\n",
        "\n",
        "# --- 2. HELPER FUNCTIONS ---\n",
        "\n",
        "def get_embedding(text):\n",
        "    return genai.embed_content(model=\"models/text-embedding-004\", content=text)[\"embedding\"]\n",
        "\n",
        "def extract_metadata(filename):\n",
        "    if \"employee_handbook\" in filename:\n",
        "        return {\"effective_date\": \"2024-01-15\", \"role_scope\": \"all_employees\", \"doc_type\": \"handbook\"}\n",
        "    if \"manager_updates\" in filename:\n",
        "        return {\"effective_date\": \"2024-06-01\", \"role_scope\": \"all_employees\", \"doc_type\": \"policy_update\"}\n",
        "    if \"intern_onboarding\" in filename:\n",
        "        return {\"effective_date\": \"2024-06-01\", \"role_scope\": \"interns\", \"doc_type\": \"role_specific\"}\n",
        "    return {\"effective_date\": \"2024-01-01\", \"role_scope\": \"all_employees\", \"doc_type\": \"general\"}\n",
        "\n",
        "def date_to_int(date_str):\n",
        "    return int(date_str.replace(\"-\", \"\"))\n",
        "\n",
        "# --- 3. INGESTION LOGIC ---\n",
        "\n",
        "def run_ingestion():\n",
        "    # Force cleanup of the specific DB path\n",
        "    # Ensure any lingering file handles are released before deletion\n",
        "    gc.collect()\n",
        "    if os.path.exists(DB_PATH):\n",
        "        try:\n",
        "            shutil.rmtree(DB_PATH)\n",
        "            print(f\"üßπ Removed old database at {DB_PATH}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Could not delete old DB (might be locked). Error: {e}. Using new path anyway.\")\n",
        "\n",
        "    client = PersistentClient(path=DB_PATH)\n",
        "    collection = client.get_or_create_collection(name=\"nebula_policies\")\n",
        "\n",
        "    filenames = [\n",
        "        \"employee_handbook_v1.txt\",\n",
        "        \"manager_updates_2024.txt\",\n",
        "        \"intern_onboarding_faq.txt\"\n",
        "    ]\n",
        "\n",
        "    ids, docs, metas, embs = [], [], [], []\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=100)\n",
        "\n",
        "    print(\"üìñ Reading files from disk...\")\n",
        "\n",
        "    for filename in filenames:\n",
        "        if not os.path.exists(filename):\n",
        "            print(f\"‚ùå ERROR: File '{filename}' not found! Please upload it to the Colab 'Files' tab.\")\n",
        "            continue\n",
        "\n",
        "        with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
        "            text = f.read()\n",
        "\n",
        "        chunks = splitter.split_text(text)\n",
        "        base_meta = extract_metadata(filename)\n",
        "\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            ids.append(f\"{filename}_{i}\")\n",
        "            docs.append(chunk)\n",
        "            metas.append({**base_meta, \"filename\": filename})\n",
        "            embs.append(get_embedding(chunk))\n",
        "\n",
        "    if len(ids) > 0:\n",
        "        collection.add(ids=ids, documents=docs, metadatas=metas, embeddings=embs)\n",
        "        print(f\"‚úÖ Ingestion Complete. {len(ids)} chunks stored in {DB_PATH}.\")\n",
        "    else:\n",
        "        print(\"‚ùå No data ingested.\")\n",
        "\n",
        "    # Explicitly delete client to release file lock\n",
        "    del client\n",
        "    gc.collect()\n",
        "\n",
        "# --- 4. RETRIEVAL LOGIC ---\n",
        "\n",
        "def retrieve_documents(query, user_role):\n",
        "    client = PersistentClient(path=DB_PATH)\n",
        "    collection = client.get_collection(\"nebula_policies\")\n",
        "\n",
        "    q_emb = get_embedding(query)\n",
        "    results = collection.query(query_embeddings=[q_emb], n_results=5)\n",
        "\n",
        "    chunks = []\n",
        "    if results['documents']:\n",
        "        for doc, meta, dist in zip(results['documents'][0], results['metadatas'][0], results['distances'][0]):\n",
        "            chunks.append({\"text\": doc, \"meta\": meta, \"dist\": dist})\n",
        "\n",
        "    ranked = []\n",
        "    target_scope = f\"{user_role}s\"\n",
        "    for ch in chunks:\n",
        "        meta = ch[\"meta\"]\n",
        "        role_match = 1 if meta[\"role_scope\"] == target_scope else 0\n",
        "        date_val = date_to_int(meta[\"effective_date\"])\n",
        "\n",
        "        ranked.append({\n",
        "            \"chunk\": ch,\n",
        "            \"score\": (-role_match, -date_val, ch[\"dist\"])\n",
        "        })\n",
        "\n",
        "    ranked = sorted(ranked, key=lambda x: x[\"score\"])\n",
        "\n",
        "    # Explicitly delete client to release file lock\n",
        "    del client\n",
        "    gc.collect()\n",
        "\n",
        "    return [r[\"chunk\"] for r in ranked[:3]]\n",
        "\n",
        "# --- 5. LLM QUERY LOGIC (Llama 3 - CRISP MODE) ---\n",
        "\n",
        "def query_llama(prompt_text):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a concise, strict, and authoritative policy assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt_text},\n",
        "    ]\n",
        "\n",
        "    prompt = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    outputs = text_pipe(prompt)\n",
        "    generated_text = outputs[0][\"generated_text\"][len(prompt):]\n",
        "    return generated_text.strip()\n",
        "\n",
        "def detect_role_local(query):\n",
        "    prompt = f\"Extract role from query (intern, employee, manager). Return ONLY the word. Query: '{query}'\"\n",
        "    role = query_llama(prompt).lower()\n",
        "    if \"intern\" in role: return \"intern\"\n",
        "    if \"manager\" in role: return \"manager\"\n",
        "    return \"employee\"\n",
        "\n",
        "# def ask_nebula_llama(query):\n",
        "#     print(\"  ...Detecting Role\")\n",
        "#     user_role = detect_role_local(query)\n",
        "\n",
        "#     print(\"  ...Retrieving Documents\")\n",
        "#     chunks = retrieve_documents(query, user_role)\n",
        "\n",
        "#     context_text = \"\"\n",
        "#     for d in chunks:\n",
        "#         context_text += f\"\\n[DOCUMENT]\\nSource: {d['meta']['filename']}\\nEffective Date: {d['meta']['effective_date']}\\nRole Scope: {d['meta']['role_scope']}\\nContent: {d['text']}\\n-------------------\\n\"\n",
        "\n",
        "#     print(\"  ...Generating Answer\")\n",
        "#     prompt = f\"\"\"\n",
        "#     You are NebulaGears' policy assistant.\n",
        "#     CURRENT USER ROLE: {user_role}\n",
        "\n",
        "#     CONFLICT RULES:\n",
        "#     1. INTERN RULE: If user is 'intern', ONLY 'interns' scope applies.\n",
        "#     2. RECENCY RULE: Newer dates override older ones.\n",
        "#     3. CITATION: Must cite the Source filename.\n",
        "\n",
        "#     STYLE GUIDELINES (STRICT):\n",
        "#     - Answer directly (Yes/No).\n",
        "#     - Be extremely concise (max 2 sentences).\n",
        "#     - Do NOT use filler phrases like \"Based on the documents\".\n",
        "#     - End with \"Source: filename.txt\" on a new line.\n",
        "\n",
        "#     CONTEXT:\n",
        "#     {context_text}\n",
        "\n",
        "#     QUESTION: \"{query}\"\n",
        "#     \"\"\"\n",
        "#     return query_llama(prompt)\n",
        "\n",
        "# --- EXECUTION ---\n",
        "run_ingestion()\n",
        "\n",
        "# print(\"\\n\" + \"=\"*50)\n",
        "# question = \"I just joined as an intern. Can I work from home?\"\n",
        "# print(f\"‚ùì User: {question}\")\n",
        "# response = ask_nebula_llama(question)\n",
        "# print(f\"ü¶ô Assistant:\\n{response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "3wZUQF0pkOqh",
        "outputId": "5745f1df-e981-4f2b-c052-62ea7c094d4a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìñ Reading files from disk...\n",
            "‚úÖ Ingestion Complete. 71 chunks stored in ./chroma_db_fresh_v2.\n",
            "\n",
            "==================================================\n",
            "‚ùì User: I just joined as an intern. Can I work from home?\n",
            "  ...Detecting Role\n",
            "  ...Retrieving Documents\n",
            "  ...Generating Answer\n",
            "ü¶ô Assistant:\n",
            "No. Interns are required to be in the office five days a week, unless a specific role-based exception is specified in an offer letter or documented by People & Culture.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# ‚ö° CELL 3: CLEAN OUTPUT VERSION (No \"Thinking\" logs)\n",
        "# ==========================================\n",
        "\n",
        "def ask_nebula_llama_clean(query):\n",
        "    # 1. Detect Role (No print statements)\n",
        "    user_role = detect_role_local(query)\n",
        "\n",
        "    # 2. Retrieve Documents (No print statements)\n",
        "    chunks = retrieve_documents(query, user_role)\n",
        "\n",
        "    context_text = \"\"\n",
        "    for d in chunks:\n",
        "        context_text += f\"\\n[DOCUMENT]\\nSource: {d['meta']['filename']}\\nEffective Date: {d['meta']['effective_date']}\\nRole Scope: {d['meta']['role_scope']}\\nContent: {d['text']}\\n-------------------\\n\"\n",
        "\n",
        "    # 3. Generate Answer\n",
        "    prompt = f\"\"\"\n",
        "    You are NebulaGears' policy assistant.\n",
        "    User Role: {user_role}\n",
        "\n",
        "    CONFLICT RULES:\n",
        "    1. Interns = Intern rules override everything.\n",
        "    2. Employees = Newer dates override older dates.\n",
        "\n",
        "    CONTEXT:\n",
        "    {context_text}\n",
        "\n",
        "    QUESTION: \"{query}\"\n",
        "\n",
        "    REQUIRED OUTPUT FORMAT:\n",
        "    [Direct Answer]\n",
        "    Source: [Filename]\n",
        "\n",
        "    YOUR ANSWER:\n",
        "    \"\"\"\n",
        "    return query_llama(prompt)\n",
        "\n",
        "# --- EXECUTION ---\n",
        "question = \"I just joined as an intern. Can I work from home?\"\n",
        "\n",
        "# Print exactly as requested: Q, Answer, Citation\n",
        "print(f\"Q: {question}\")\n",
        "print(ask_nebula_llama_clean(question))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "YwEKH1MQkOnr",
        "outputId": "e9350a9a-790c-461e-ec3a-2d52101ce2cd"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: I just joined as an intern. Can I work from home?\n",
            "**No**, you cannot work from home as an intern. According to the Core Policy ‚Äî Office Presence, interns are required to be in the office 5 days a week for the duration of their internship to maximize mentorship. No remote work is permitted for interns. (Source: intern_onboarding_faq.txt)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Do I need approval to work from home?\"\n",
        "\n",
        "# Print exactly as requested: Q, Answer, Citation\n",
        "print(f\"Q: {question}\")\n",
        "print(ask_nebula_llama_clean(question))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "pRDvp3gPz9n3",
        "outputId": "34a2218d-61b2-489b-a68e-825b80335a79"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: Do I need approval to work from home?\n",
            "**Answer:** No, you do not need approval to work from home. However, please note that:\n",
            "\n",
            "* As an intern, you are subject to the intern rules, which override all other rules. According to the intern_onboarding_faq.txt, you should notify People & Culture and your manager if you have a medical need or other compelling personal situation requiring periodic remote work.\n",
            "* As a manager, you should consult the manager_updates_2024.txt, which clarifies that temporary remote working for short windows (e.g., travel or emergencies) requires People & Culture consent.\n",
            "* For employees, the employee_handbook_v1.txt FAQ states that the Work From Anywhere program does not require prior approval for remote days, but managers should be informed of extended travel or timezone changes impacting team collaboration.\n",
            "\n",
            "**Source:** intern_onboarding_faq.txt, manager_updates_2024.txt, and employee_handbook_v1.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# üß™ FINAL TEST SUITE (15 QUESTIONS)\n",
        "# ==========================================\n",
        "\n",
        "# 1. Define the 15 Questions\n",
        "test_data = {\n",
        "    \"GROUP 1: INTERN QUESTIONS (Strict Override)\": [\n",
        "        \"I just joined as an intern. Can I work from home?\",\n",
        "        \"As a new intern, can I work remotely 3 days a week like employees?\",\n",
        "        \"Do interns need manager approval for remote work?\"\n",
        "    ],\n",
        "    \"GROUP 2: EMPLOYEE QUESTIONS (Manager Update Override)\": [\n",
        "        \"Do employees still follow the Work From Anywhere policy?\",\n",
        "        \"Do I need approval to work from home?\",\n",
        "        \"Can employees work remotely 100% of the time?\"\n",
        "    ],\n",
        "    \"GROUP 3: MANAGER QUESTIONS (Policy Enforcement)\": [\n",
        "        \"As a manager, can I approve 5 remote days per week?\",\n",
        "        \"Which days must employees be in the HQ office?\",\n",
        "        \"Can I allow my entire team to work remotely all week?\"\n",
        "    ],\n",
        "    \"GROUP 4: NEGATION & LOGIC (Tricky phrasing)\": [\n",
        "        \"Do I NOT need approval to work remotely?\",\n",
        "        \"Is it NOT mandatory to come to office on certain days?\",\n",
        "        \"Is it true that interns do NOT have to come every day?\"\n",
        "    ],\n",
        "    \"GROUP 5: CONFLICT RESOLUTION META-QUESTIONS\": [\n",
        "        \"Which document do I follow if handbook says no approval is needed but manager update says approval is required?\",\n",
        "        \"Does the manager update apply to interns?\",\n",
        "        \"If two policies conflict, which one wins?\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# 2. Select the active RAG function (Gemini or Llama)\n",
        "# We check which function you ran last\n",
        "if 'ask_nebula' in globals():\n",
        "    runner_fn = ask_nebula\n",
        "elif 'ask_nebula_llama_clean' in globals():\n",
        "    runner_fn = ask_nebula_llama_clean\n",
        "elif 'ask_nebula_llama' in globals():\n",
        "    runner_fn = ask_nebula_llama\n",
        "else:\n",
        "    print(\"‚ùå ERROR: No RAG function found! Please run your RAG Logic cell first.\")\n",
        "    runner_fn = None\n",
        "\n",
        "# 3. Run the Loop\n",
        "if runner_fn:\n",
        "    print(f\"üöÄ STARTING TEST RUN [Using: {runner_fn.__name__}]\")\n",
        "\n",
        "    for group_name, questions in test_data.items():\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"üìÇ {group_name}\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        for q in questions:\n",
        "            print(f\"\\n‚ùì Q: {q}\")\n",
        "\n",
        "            try:\n",
        "                # Call the AI\n",
        "                ans = runner_fn(q)\n",
        "                print(f\"ü§ñ A:\\n{ans.strip()}\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Error: {e}\")\n",
        "\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "    print(\"\\n‚úÖ TEST SUITE COMPLETE.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6I07tkjJkOlA",
        "outputId": "714764b2-58f9-427e-9826-7eac6ea8af98"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ STARTING TEST RUN [Using: ask_nebula_llama_clean]\n",
            "\n",
            "======================================================================\n",
            "üìÇ GROUP 1: INTERN QUESTIONS (Strict Override)\n",
            "======================================================================\n",
            "\n",
            "‚ùì Q: I just joined as an intern. Can I work from home?\n",
            "ü§ñ A:\n",
            "**No**, you cannot work from home as an intern. According to the Core Policy ‚Äî Office Presence, interns are required to be in the office 5 days a week for the duration of their internship to maximize mentorship. No remote work is permitted for interns. (Source: intern_onboarding_faq.txt)\n",
            "----------------------------------------\n",
            "\n",
            "‚ùì Q: As a new intern, can I work remotely 3 days a week like employees?\n",
            "ü§ñ A:\n",
            "**Direct Answer:** No, as a new intern, you are not allowed to work remotely 3 days a week like employees. According to the Core Policy ‚Äî Office Presence, interns are required to be in the office 5 days a week for the duration of their internship to maximize mentorship. No remote work is permitted for interns.\n",
            "----------------------------------------\n",
            "\n",
            "‚ùì Q: Do interns need manager approval for remote work?\n",
            "ü§ñ A:\n",
            "**Direct Answer:** No, interns do not need manager approval for remote work. According to the policy, interns are not permitted to work remotely at all during the internship, unless a specific role-based exception is specified in an offer letter or documented by People & Culture.\n",
            "\n",
            "**Source:** intern_onboarding_faq.txt\n",
            "----------------------------------------\n",
            "\n",
            "======================================================================\n",
            "üìÇ GROUP 2: EMPLOYEE QUESTIONS (Manager Update Override)\n",
            "======================================================================\n",
            "\n",
            "‚ùì Q: Do employees still follow the Work From Anywhere policy?\n",
            "ü§ñ A:\n",
            "**Direct Answer:** Yes, employees still follow the Work From Anywhere policy.\n",
            "\n",
            "**Source:** employee_handbook_v1.txt\n",
            "----------------------------------------\n",
            "\n",
            "‚ùì Q: Do I need approval to work from home?\n",
            "ü§ñ A:\n",
            "**Answer:** No, you do not need approval to work from home. However, please note that there are specific rules and exceptions to consider.\n",
            "\n",
            "**Source:** employee_handbook_v1.txt\n",
            "----------------------------------------\n",
            "\n",
            "‚ùì Q: Can employees work remotely 100% of the time?\n",
            "ü§ñ A:\n",
            "**Answer:** No\n",
            "\n",
            "**Source:** employee_handbook_v1.txt\n",
            "----------------------------------------\n",
            "\n",
            "======================================================================\n",
            "üìÇ GROUP 3: MANAGER QUESTIONS (Policy Enforcement)\n",
            "======================================================================\n",
            "\n",
            "‚ùì Q: As a manager, can I approve 5 remote days per week?\n",
            "ü§ñ A:\n",
            "**Direct Answer:** No.\n",
            "\n",
            "**Source:** manager_updates_2024.txt\n",
            "----------------------------------------\n",
            "\n",
            "‚ùì Q: Which days must employees be in the HQ office?\n",
            "ü§ñ A:\n",
            "Based on the provided documents, the answer is:\n",
            "\n",
            "**Tuesday and Thursday**\n",
            "\n",
            "Source: manager_updates_2024.txt\n",
            "----------------------------------------\n",
            "\n",
            "‚ùì Q: Can I allow my entire team to work remotely all week?\n",
            "ü§ñ A:\n",
            "**Direct Answer:** No.\n",
            "\n",
            "**Source:** manager_updates_2024.txt\n",
            "----------------------------------------\n",
            "\n",
            "======================================================================\n",
            "üìÇ GROUP 4: NEGATION & LOGIC (Tricky phrasing)\n",
            "======================================================================\n",
            "\n",
            "‚ùì Q: Do I NOT need approval to work remotely?\n",
            "ü§ñ A:\n",
            "**Answer:** No.\n",
            "\n",
            "**Source:** employee_handbook_v1.txt\n",
            "----------------------------------------\n",
            "\n",
            "‚ùì Q: Is it NOT mandatory to come to office on certain days?\n",
            "ü§ñ A:\n",
            "**Direct Answer:** No, it is not mandatory to come to the office on certain days.\n",
            "\n",
            "**Source:** employee_handbook_v1.txt\n",
            "----------------------------------------\n",
            "\n",
            "‚ùì Q: Is it true that interns do NOT have to come every day?\n",
            "ü§ñ A:\n",
            "**Direct Answer:** No, according to the policy, interns are required to be in the office five days a week, unless a specific role-based exception is specified in an offer letter or documented by People & Culture.\n",
            "\n",
            "**Source:** intern_onboarding_faq.txt\n",
            "----------------------------------------\n",
            "\n",
            "======================================================================\n",
            "üìÇ GROUP 5: CONFLICT RESOLUTION META-QUESTIONS\n",
            "======================================================================\n",
            "\n",
            "‚ùì Q: Which document do I follow if handbook says no approval is needed but manager update says approval is required?\n",
            "ü§ñ A:\n",
            "[Direct Answer]\n",
            "Source: manager_updates_2024.txt\n",
            "\n",
            "According to the Manager Update 2024, if there is a conflict between the Employee Handbook and the Manager Update, the Manager Update takes precedence. Specifically, the update clarifies and limits the \"Work From Anywhere\" guidance, requiring approval for certain policies. Therefore, if the handbook states no approval is needed, but the manager update requires approval, you should follow the manager update.\n",
            "----------------------------------------\n",
            "\n",
            "‚ùì Q: Does the manager update apply to interns?\n",
            "ü§ñ A:\n",
            "**Answer:** Yes, the manager update applies to interns.\n",
            "\n",
            "**Source:** manager_updates_2024.txt\n",
            "----------------------------------------\n",
            "\n",
            "‚ùì Q: If two policies conflict, which one wins?\n",
            "ü§ñ A:\n",
            "According to the CONFLICT RULES, if two policies conflict, the Intern rules override everything. Therefore, if an intern is involved, the intern-specific policy (intern_onboarding_faq.txt) takes precedence.\n",
            "\n",
            "However, if the conflict involves employees, the newer date takes precedence. Since both manager_updates_2024.txt documents have the same effective date (2024-06-01), they are considered tied. In this case, the scope of the documents comes into play. The manager_updates_2024.txt document has a broader scope, applying to all employees, including contractors when explicitly contracted for remote flexibility. Therefore, the more recent policy (manager_updates_2024.txt) takes precedence.\n",
            "\n",
            "In summary:\n",
            "\n",
            "* If an intern is involved, the intern_onboarding_faq.txt takes precedence.\n",
            "* If employees are involved, the newer date (manager_updates_2024.txt) takes precedence.\n",
            "\n",
            "[Direct Answer]\n",
            "Source: CONFLICT RULES\n",
            "----------------------------------------\n",
            "\n",
            "‚úÖ TEST SUITE COMPLETE.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QDfTRJGdozrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iGNCj2aDozo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iF5RScsGozmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9qV7kF3UozkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VVlkKpAcozh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4iJdDGCMozfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O0QVl43cozc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qbYoLeVcozah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PHKQP6qyozX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "47hVHfLEozVf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}